#!/usr/bin/env python3
"""
è°ƒè¯•é¡µé¢ç»“æ„çš„è„šæœ¬
"""

import requests
import logging
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
import random

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def debug_page_structure():
    """è°ƒè¯•é¡µé¢ç»“æ„"""
    print("ğŸ” è°ƒè¯•é¡µé¢ç»“æ„...")
    
    blogger_url = "https://www.toutiao.com/c/user/token/MS4wLjABAAAAu8TLqbwurnpNAkvSb1SB60loMjrybIwxT3py56-uKRM/?source=profile&tab=article"
    
    print(f"è®¿é—®URL: {blogger_url}")
    
    # è®¾ç½®è¯·æ±‚å¤´
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0'
    ]
    
    headers = {
        'User-Agent': random.choice(user_agents),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0',
        'DNT': '1',
    }
    
    session = requests.Session()
    session.headers.update(headers)
    
    try:
        response = session.get(blogger_url, timeout=15)
        print(f"çŠ¶æ€ç : {response.status_code}")
        print(f"å“åº”é•¿åº¦: {len(response.text)}")
        
        if response.status_code == 200:
            # ä¿å­˜åŸå§‹HTML
            with open('debug_page_structure.html', 'w', encoding='utf-8') as f:
                f.write(response.text)
            print("âœ… é¡µé¢å†…å®¹å·²ä¿å­˜åˆ° debug_page_structure.html")
            
            # è§£æé¡µé¢
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥çœ‹é¡µé¢æ ‡é¢˜
            title = soup.find('title')
            if title:
                print(f"\né¡µé¢æ ‡é¢˜: {title.get_text()}")
            
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ–‡ç« é“¾æ¥
            print("\nğŸ” æŸ¥æ‰¾æ–‡ç« é“¾æ¥...")
            
            # æ–¹æ³•1ï¼šæŸ¥æ‰¾class="title"çš„é“¾æ¥
            title_links = soup.find_all('a', class_='title')
            print(f"æ‰¾åˆ° {len(title_links)} ä¸ªclass='title'çš„é“¾æ¥")
            for i, link in enumerate(title_links[:3]):
                print(f"  {i+1}. href: {link.get('href')}")
                print(f"     text: {link.get_text(strip=True)[:50]}...")
                print(f"     aria-label: {link.get('aria-label', '')[:50]}...")
            
            # æ–¹æ³•2ï¼šæŸ¥æ‰¾æ‰€æœ‰åŒ…å«/article/çš„é“¾æ¥
            article_links = soup.find_all('a', href=lambda x: x and '/article/' in x)
            print(f"\næ‰¾åˆ° {len(article_links)} ä¸ªåŒ…å«'/article/'çš„é“¾æ¥")
            for i, link in enumerate(article_links[:5]):
                print(f"  {i+1}. href: {link.get('href')}")
                print(f"     text: {link.get_text(strip=True)[:50]}...")
                print(f"     class: {link.get('class')}")
                print(f"     aria-label: {link.get('aria-label', '')[:50]}...")
            
            # æ–¹æ³•3ï¼šæŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
            all_links = soup.find_all('a', href=True)
            print(f"\næ€»å…±æ‰¾åˆ° {len(all_links)} ä¸ªé“¾æ¥")
            
            # æŸ¥æ‰¾å¯èƒ½çš„æ–‡ç« å®¹å™¨
            print("\nğŸ” æŸ¥æ‰¾æ–‡ç« å®¹å™¨...")
            
            # æŸ¥æ‰¾å¯èƒ½çš„æ–‡ç« å¡ç‰‡
            article_cards = soup.find_all('div', class_=lambda x: x and ('article' in str(x).lower() or 'card' in str(x).lower() or 'feed' in str(x).lower()))
            print(f"æ‰¾åˆ° {len(article_cards)} ä¸ªå¯èƒ½çš„æ–‡ç« å¡ç‰‡")
            for i, card in enumerate(article_cards[:3]):
                print(f"  {i+1}. class: {card.get('class')}")
            
            # æŸ¥æ‰¾scriptæ ‡ç­¾ä¸­çš„æ•°æ®
            print("\nğŸ” æŸ¥æ‰¾scriptæ ‡ç­¾ä¸­çš„æ•°æ®...")
            scripts = soup.find_all('script')
            print(f"æ‰¾åˆ° {len(scripts)} ä¸ªscriptæ ‡ç­¾")
            
            # æŸ¥æ‰¾å¯èƒ½åŒ…å«æ•°æ®çš„scriptæ ‡ç­¾
            for i, script in enumerate(scripts):
                if script.string and ('article' in script.string.lower() or 'data' in script.string.lower()):
                    print(f"Script {i+1} å¯èƒ½åŒ…å«æ•°æ®: {script.string[:200]}...")
                    
                    # å¦‚æœåŒ…å«æ–‡ç« æ•°æ®ï¼Œä¿å­˜åˆ°æ–‡ä»¶
                    if 'article' in script.string.lower():
                        with open(f'debug_script_{i+1}.txt', 'w', encoding='utf-8') as f:
                            f.write(script.string)
                        print(f"  å·²ä¿å­˜åˆ° debug_script_{i+1}.txt")
            
            # æŸ¥æ‰¾ç‰¹å®šçš„å…ƒç´ 
            print("\nğŸ” æŸ¥æ‰¾ç‰¹å®šå…ƒç´ ...")
            
            # æŸ¥æ‰¾å¯èƒ½çš„æ–‡ç« åˆ—è¡¨å®¹å™¨
            containers = soup.find_all(['div', 'section', 'ul'], class_=lambda x: x and any(keyword in str(x).lower() for keyword in ['list', 'content', 'main', 'wrapper']))
            print(f"æ‰¾åˆ° {len(containers)} ä¸ªå¯èƒ½çš„å®¹å™¨")
            for i, container in enumerate(containers[:3]):
                print(f"  {i+1}. æ ‡ç­¾: {container.name}, class: {container.get('class')}")
                # æŸ¥æ‰¾å®¹å™¨å†…çš„é“¾æ¥
                inner_links = container.find_all('a', href=True)
                print(f"     å†…éƒ¨é“¾æ¥æ•°: {len(inner_links)}")
            
        else:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
            print(f"å“åº”å†…å®¹: {response.text[:500]}...")
            
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")

if __name__ == '__main__':
    debug_page_structure()
